# Creating your own model

This guide will walk you trough on how to prepare and create a dataset as well as how to train your own model

## Data preparation

If you have already collected images of a dartboard and the darts and you want to train a model on them or evaluate an existing one, the images have to be annotated.

This project uses the open source labeling software LabelStudio. Locally running the LabelStudio instance is preferred an can easily be achieved by running the docker-compose in this project with `docker compose up`.
This will spin up 2 Containers. One for running the Label studio software locally (default to `localhost:8080`) and one for an LabelStudio ml backend to run prediction for pre labeling new data.

After running the docker compose you should be able to access the LS in your browser. First you need to sign in to an account. Either create an account with any email and password. No need to provide a functional email or use the default user with email and password : amin@admin.com.  
After authenticating you can create a project manually or run `python label-studio-setup/project.py Scored` to create a project with the necessary labeling config, ml backend and local storage configuration.

> For running the scripts in the `label-studio-setup` folder you need to define the env variables for the LS URL and an API Key which can retrieved by opening LS, navigating to your account settings, Access Token.

The folder that contains all of the collect images is by default located in `./dataset/`. You can change this by editing the env var `DATASET_PATH` to your desired location. All the images in this folder will be served with LS as long as the local Storage is correctly setup.

### LS ml backend

To speed up the labeling process LS provided support for an ml backend which runs a model on the images and pre-annotates the images.
The annotator now only needs to correct the images with bad predictions.
To uses this run `python label-studio-setup/connect-ml-backend.py` which will automatically connect the ml backend running on another container to your project.

By default the ml backend expects a model in the `models/` which it will use to make predictions. Which model it will use can be configured in the Projects Labeling interface. Change the `model_path` value to any model relative to the `models/` folder.

> If the ML Backend container crashes with this error message:
>
> ```
> exec /app/start.sh : No such file or directory
> exited with code 1
> ```
>
> Adjust your git config `git config --global core.autocrlf false` and clone the repo again.
> See more about this error [here](https://github.com/HumanSignal/label-studio-ml-backend?tab=readme-ov-file#troubleshooting-docker-build-on-windows)

## Labeling

To label your data, selected the image and press 'Label x Task'.
If you only want to label or e.g. export a specific dateset version you can add a filter on the 'Storage Filename' column with e.g. contains '/v1/' if this is the subdir where all of your images from the first version are located.
When the ml backend is enabled it will take some time, before you can label the image, for the prediction to arrive. This can slow down the process so it is recommend to batch predict all new images by just selecting them and pressing 'retrieve predictions'.
When labeling the BB is only necessary for grouping the keypoints. Later when exporting your dataset the BB will be autogenerated from the keypoints. So no need for perfect BB's.

## Creating a Dataset

After labeling the images we need to create a dataset on which the model can be trained on. Since this project uses YOLO, we need to follow the yolo dataset structure. Sadly LS only supports a limited number of export formats. You have to export the data in the LS JSON format and use the conversion script included in this project to transform the JSON annotations into YOLO format.
This can be done by running the `dataset` script like this in the root directory of this project:

` python .\scripts\dataset.py --keypoint-order top right bottom left --keypoint-order dart dart-flight --classes dartboard dart --split .7 .2 .1 --clear-existing <path_to_exported_annotations.json>`

> Some of the parameters have to be changed if the LS view is defined with different keypoint or class names as well if you want different splits or outputs. Refer to `python .\scripts\dataset.py --help` for more information about the parameters.

## Training a model

Training the model on the dataset requires computational power.
If you don't have a dedicated GPU it is recommended to use Google Colab to train your model, which provide a free tier for accessing GPU's.
A dedicated [notebook](notebooks/colab_train.ipynb) for the training on Google Colab can be used. Walk through the notebook to get more insights about the training process.
